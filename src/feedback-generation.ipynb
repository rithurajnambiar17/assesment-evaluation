{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T20:02:39.142255Z","iopub.status.busy":"2022-05-15T20:02:39.141416Z","iopub.status.idle":"2022-05-15T20:02:52.059697Z","shell.execute_reply":"2022-05-15T20:02:52.058788Z","shell.execute_reply.started":"2022-05-15T20:02:39.142204Z"},"trusted":true},"outputs":[],"source":["!pip3 install tez"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T20:02:52.061605Z","iopub.status.busy":"2022-05-15T20:02:52.061354Z","iopub.status.idle":"2022-05-15T20:02:59.267879Z","shell.execute_reply":"2022-05-15T20:02:59.266880Z","shell.execute_reply.started":"2022-05-15T20:02:52.061565Z"},"trusted":true},"outputs":[],"source":["import gc\n","gc.enable()\n","\n","import sys\n","sys.path.append(\"../input/tez-lib/\")\n","\n","import os\n","import numpy as np\n","import pandas as pd\n","import tez\n","import torch\n","import torch.nn as nn\n","from joblib import Parallel, delayed\n","from transformers import AutoConfig, AutoModel, AutoTokenizer"]},{"cell_type":"code","execution_count":6,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-05-15T20:03:11.012133Z","iopub.status.busy":"2022-05-15T20:03:11.011866Z","iopub.status.idle":"2022-05-15T20:03:11.022604Z","shell.execute_reply":"2022-05-15T20:03:11.021986Z","shell.execute_reply.started":"2022-05-15T20:03:11.012098Z"},"trusted":true},"outputs":[],"source":["target_id_map = {\n","    \"B-Lead\": 0,\n","    \"I-Lead\": 1,\n","    \"B-Position\": 2,\n","    \"I-Position\": 3,\n","    \"B-Evidence\": 4,\n","    \"I-Evidence\": 5,\n","    \"B-Claim\": 6,\n","    \"I-Claim\": 7,\n","    \"B-Concluding Statement\": 8,\n","    \"I-Concluding Statement\": 9,\n","    \"B-Counterclaim\": 10,\n","    \"I-Counterclaim\": 11,\n","    \"B-Rebuttal\": 12,\n","    \"I-Rebuttal\": 13,\n","    \"O\": 14,\n","    \"PAD\": -100,\n","}\n","\n","id_target_map = {v: k for k, v in target_id_map.items()}\n","\n","class args1:\n","    input_path = \"../input/feedback-prize-2021/\"\n","    model = \"../input/longformerlarge4096/longformer-large-4096/\"\n","    tez_model= \"../input/fblongformerlarge1536/\"\n","    output = \".\"\n","    batch_size = 8\n","    max_len = 4096\n","    \n","class args2:\n","    input_path = \"../input/feedback-prize-2021/\"\n","    model = \"../input/longformerlarge4096/longformer-large-4096/\"\n","    tez_model= \"../input/tez-fb-large/\"\n","    output = \".\"\n","    batch_size = 8\n","    max_len = 4096"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T20:03:11.736982Z","iopub.status.busy":"2022-05-15T20:03:11.736180Z","iopub.status.idle":"2022-05-15T20:03:11.744073Z","shell.execute_reply":"2022-05-15T20:03:11.743376Z","shell.execute_reply.started":"2022-05-15T20:03:11.736936Z"},"trusted":true},"outputs":[],"source":["class FeedbackDataset:\n","    def __init__(self, samples, max_len, tokenizer):\n","        self.samples = samples\n","        self.max_len = max_len\n","        self.tokenizer = tokenizer\n","        self.length = len(samples)\n","\n","    def __len__(self):\n","        return self.length\n","\n","    def __getitem__(self, idx):\n","        input_ids = self.samples[idx][\"input_ids\"]\n","\n","        # add start token id to the input_ids\n","        input_ids = [self.tokenizer.cls_token_id] + input_ids\n","\n","        if len(input_ids) > self.max_len - 1:\n","            input_ids = input_ids[: self.max_len - 1]\n","\n","        # add end token id to the input_ids\n","        input_ids = input_ids + [self.tokenizer.sep_token_id]\n","        attention_mask = [1] * len(input_ids)\n","\n","        return {\n","            \"ids\": input_ids,\n","            \"mask\": attention_mask,\n","        }"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T20:03:12.301203Z","iopub.status.busy":"2022-05-15T20:03:12.300776Z","iopub.status.idle":"2022-05-15T20:03:12.310851Z","shell.execute_reply":"2022-05-15T20:03:12.310181Z","shell.execute_reply.started":"2022-05-15T20:03:12.301172Z"},"trusted":true},"outputs":[],"source":["class Collate:\n","    def __init__(self, tokenizer):\n","        self.tokenizer = tokenizer\n","\n","    def __call__(self, batch):\n","        output = dict()\n","        output[\"ids\"] = [sample[\"ids\"] for sample in batch]\n","        output[\"mask\"] = [sample[\"mask\"] for sample in batch]\n","\n","        # calculate max token length of this batch\n","        batch_max = max([len(ids) for ids in output[\"ids\"]])\n","\n","        # add padding\n","        if self.tokenizer.padding_side == \"right\":\n","            output[\"ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"ids\"]]\n","            output[\"mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"mask\"]]\n","        else:\n","            output[\"ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"ids\"]]\n","            output[\"mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"mask\"]]\n","\n","        # convert to tensors\n","        output[\"ids\"] = torch.tensor(output[\"ids\"], dtype=torch.long)\n","        output[\"mask\"] = torch.tensor(output[\"mask\"], dtype=torch.long)\n","\n","        return output"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T20:03:12.641133Z","iopub.status.busy":"2022-05-15T20:03:12.640862Z","iopub.status.idle":"2022-05-15T20:03:12.649929Z","shell.execute_reply":"2022-05-15T20:03:12.648916Z","shell.execute_reply.started":"2022-05-15T20:03:12.641095Z"},"trusted":true},"outputs":[],"source":["class FeedbackModel(tez.Model):\n","    def __init__(self, model_name, num_labels):\n","        super().__init__()\n","        self.model_name = model_name\n","        self.num_labels = num_labels\n","        config = AutoConfig.from_pretrained(model_name)\n","\n","        hidden_dropout_prob: float = 0.1\n","        layer_norm_eps: float = 1e-7\n","        config.update(\n","            {\n","                \"output_hidden_states\": True,\n","                \"hidden_dropout_prob\": hidden_dropout_prob,\n","                \"layer_norm_eps\": layer_norm_eps,\n","                \"add_pooling_layer\": False,\n","            }\n","        )\n","        self.transformer = AutoModel.from_config(config)\n","        self.output = nn.Linear(config.hidden_size, self.num_labels)\n","\n","    def forward(self, ids, mask):\n","        transformer_out = self.transformer(ids, mask)\n","        sequence_output = transformer_out.last_hidden_state\n","        logits = self.output(sequence_output)\n","        logits = torch.softmax(logits, dim=-1)\n","        return logits, 0, {}"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T20:03:17.405081Z","iopub.status.busy":"2022-05-15T20:03:17.404815Z","iopub.status.idle":"2022-05-15T20:03:17.414907Z","shell.execute_reply":"2022-05-15T20:03:17.414002Z","shell.execute_reply.started":"2022-05-15T20:03:17.405053Z"},"trusted":true},"outputs":[],"source":["def _prepare_test_data_helper(args, tokenizer, ids):\n","    test_samples = []\n","    for idx in ids:\n","        filename = os.path.join(args.input_path, \"test\", idx + \".txt\")\n","        with open(filename, \"r\") as f:\n","            text = f.read()\n","\n","        encoded_text = tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=False,\n","            return_offsets_mapping=True,\n","        )\n","        input_ids = encoded_text[\"input_ids\"]\n","        offset_mapping = encoded_text[\"offset_mapping\"]\n","\n","        sample = {\n","            \"id\": idx,\n","            \"input_ids\": input_ids,\n","            \"text\": text,\n","            \"offset_mapping\": offset_mapping,\n","        }\n","\n","        test_samples.append(sample)\n","    return test_samples\n","\n","\n","def prepare_test_data(df, tokenizer, args):\n","    test_samples = []\n","    ids = df[\"id\"].unique()\n","    ids_splits = np.array_split(ids, 4)\n","\n","    results = Parallel(n_jobs=4, backend=\"multiprocessing\")(\n","        delayed(_prepare_test_data_helper)(args, tokenizer, idx) for idx in ids_splits\n","    )\n","    for result in results:\n","        test_samples.extend(result)\n","\n","    return test_samples"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T20:04:25.364001Z","iopub.status.busy":"2022-05-15T20:04:25.363141Z","iopub.status.idle":"2022-05-15T20:04:25.864110Z","shell.execute_reply":"2022-05-15T20:04:25.862632Z","shell.execute_reply.started":"2022-05-15T20:04:25.363960Z"},"trusted":true},"outputs":[],"source":["df = pd.read_csv(os.path.join(\"../input/feedback-prize-2021/\", \"sample_submission.csv\"))\n","df_ids = df[\"id\"].unique()\n","\n","tokenizer = AutoTokenizer.from_pretrained(args1.model)\n","test_samples = prepare_test_data(df, tokenizer, args1)\n","collate = Collate(tokenizer=tokenizer)\n","\n","raw_preds = []\n","for fold_ in range(10):\n","    current_idx = 0\n","    test_dataset = FeedbackDataset(test_samples, args1.max_len, tokenizer)\n","    \n","    if fold_ < 5:\n","        model = FeedbackModel(model_name=args1.model, num_labels=len(target_id_map) - 1)\n","        model.load(os.path.join(args1.tez_model, f\"model_{fold_}.bin\"), weights_only=True)\n","        preds_iter = model.predict(test_dataset, batch_size=args1.batch_size, n_jobs=-1, collate_fn=collate)\n","    else:\n","        model = FeedbackModel(model_name=args2.model, num_labels=len(target_id_map) - 1)\n","        model.load(os.path.join(args2.tez_model, f\"model_{fold_-5}.bin\"), weights_only=True)\n","        preds_iter = model.predict(test_dataset, batch_size=args2.batch_size, n_jobs=-1, collate_fn=collate)\n","        \n","    current_idx = 0\n","    \n","    for preds in preds_iter:\n","        preds = preds.astype(np.float16)\n","        preds = preds / 10\n","        if fold_ == 0:\n","            raw_preds.append(preds)\n","        else:\n","            raw_preds[current_idx] += preds\n","            current_idx += 1\n","    torch.cuda.empty_cache()\n","    gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
